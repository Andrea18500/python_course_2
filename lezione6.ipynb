{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634a5750",
   "metadata": {},
   "source": [
    "# Pandas ðŸ¼ðŸ¼ðŸ¼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5371ce",
   "metadata": {},
   "source": [
    "### Miglior strumento di data wrangling\n",
    "\n",
    "Pandas Ã¨ ideale per il data wrangling, ovvero il processo di trasformazione e pulizia dei dati grezzi per analisi e modelli, mentre NumPy/array eccelle nei calcoli intensivi una volta che il dataset Ã¨ pronto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c465a",
   "metadata": {},
   "source": [
    "| Esigenza tipica                                                  | FunzionalitÃ  Pandas                     | Quando serve                                                                |\n",
    "| ---------------------------------------------------------------- | --------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| 1. **Importare** formati eterogenei (CSV, Excel, TSV, JSON, SQL) | `read_*()`                              | Ogni volta che i dati provengono da laboratori, EHR, registry o altri dispositivi |\n",
    "| 2. **Pulire** dati sporchi o parziali                            | `dropna`, `fillna`, `astype`, `replace` | Prima di qualsiasi statistica o di usarli per Machine Learning                                          |\n",
    "| 3. **Ristrutturare** il dataset (modificarne la struttura per facilitarne lâ€™analisi o per adattarlo a specifiche esigenze)                                  | `melt`, `pivot`, `stack/unstack`        | Per allineare o sistemare misure longitudinali, matrici di espressione...                 |\n",
    "| 4. **Filtrare & creare sottogruppi**                                    | Boolean indexing, `query`, `groupby`    | Focus su un fenotipo, un trattamento, un gene...                               |\n",
    "| 5. **Aggregare & statistiche rapide**                            | `groupby().agg()`, `describe`           | Report clinici, cruscotti QC (Quality Control)...                                                |\n",
    "| 6. **Merge/Join** di piÃ¹ fonti                                   | `merge`, `concat`                       | Integrare imaging + clinica + omics (Dati omici â†’ Genomica, trascrittomica, proteomica, metabolomica)                                         |\n",
    "| 7. **Time series per il monitoraggio continuo**                                               | `to_datetime`, `resample`, `rolling`    | Wearable, follow-up, Analisi di segnali continui in terapia intensiva (ICU waveform analysis)...                                           |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308c465a",
   "metadata": {},
   "source": [
    "### PerchÃ© non basta usare liste Python o array NumPy al posto di Pandas?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f5f71",
   "metadata": {},
   "source": [
    "| Esigenza tipica nei dati biomedici                     | Liste Python                                | NumPy array                                           | Pandas DataFrame                                |\n",
    "| ------------------------------------------------------ | ------------------------------------------- | ----------------------------------------------------- | ----------------------------------------------- |\n",
    "| **Etichette (ID paziente, nome gene, tempo)**          | âŒ gestite â€œa manoâ€ con strutture ausiliarie | âŒ non esistono; solo indici interi                    | âœ… `index`, `columns`, gerarchici (`MultiIndex`) |\n",
    "| **Gestire dati eterogenei** (numeri + stringhe + date)         | âœ… ma zero funzioni vettoriali               | âš ï¸ possibile con `dtype=object`, ma perde in performance | âœ… colonne con tipi diversi, ottimizzate         |\n",
    "| **Valori mancanti** (`NaN`, `None`, sentinel)          | âŒ serve logica ad-hoc                       | âš ï¸ `np.nan` solo per float; complicato per tipi misti | âœ… `isna` per identificare valori mancanti, `fillna`per riempire valori mancanti, propagazione coerente per propagare il valore precedente o successivo nelle celle vuote.       |\n",
    "| **Operazioni â€œSQL-likeâ€** (`groupby`, `join`, `pivot`) | âŒ codice manuale e ciclo esplicito          | âŒ non previste                                        | âœ… una riga di codice idiomatica (ovvero con funzioni intuitive e efficienti)                |\n",
    "| **Time series**             | âŒ da implementare                           | âš ï¸ serve `np.searchsorted`/for loop                   | âœ… API dedicate (`resample` per aggregazione a intervalli temporali, `rolling` per applicare operazioni statistiche su una finestra mobile di dati, `shift` per confrontare valori tra istanti di tempo diversi) |\n",
    "| **I/O (input/output) nativo** (CSV, Excel, SQL, Parquet)              | âŒ librerie esterne, parsing manuale         | âŒ idem                                                | âœ… `read_*`, `to_*` tutte funzioni one-liner                    |\n",
    "| **LeggibilitÃ  notebook/paper**                         | ðŸ˜ spesso 3-4 strutture parallele           | ðŸ˜ array senza intestazioni esplicite                             | ðŸ˜€ tabelle auto-formattate                      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf36aa",
   "metadata": {},
   "source": [
    "### ðŸ¥ 1 | Analisi di dataset clinici multifattoriali\n",
    "(= studiare dati sanitari considerando piÃ¹ variabili contemporaneamente, come etÃ , sesso, genetica, stile di vita, parametri fisiologici ...)\n",
    "\n",
    "Scenario:\n",
    "Analisi di parametri clinici di pazienti con diverse diagnosi per identificare correlazioni e pattern significativi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0190fa75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in ./.venv/lib/python3.12/site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Statistiche per gruppo diagnostico:\n",
      "                   etÃ               glicemia            pressione_sistolica  \\\n",
      "                  mean       std        mean        std                mean   \n",
      "diagnosi                                                                      \n",
      "diabete      66.666667  4.509250  147.333333  20.526406          156.666667   \n",
      "pre-diabete  55.000000  3.000000  110.000000   5.000000          140.000000   \n",
      "sano         43.250000  4.787136   93.250000   4.272002          118.250000   \n",
      "\n",
      "                      colesterolo             \n",
      "                  std        mean        std  \n",
      "diagnosi                                      \n",
      "diabete      7.637626  255.000000  13.228757  \n",
      "pre-diabete  5.000000  228.333333   7.637626  \n",
      "sano         6.238322  182.500000   6.454972  \n",
      "\n",
      "Matrice di correlazione:\n",
      "                          etÃ   glicemia  pressione_sistolica  colesterolo\n",
      "etÃ                   1.000000  0.948095             0.992946     0.970972\n",
      "glicemia             0.948095  1.000000             0.943507     0.923658\n",
      "pressione_sistolica  0.992946  0.943507             1.000000     0.989907\n",
      "colesterolo          0.970972  0.923658             0.989907     1.000000\n",
      "\n",
      "Pazienti con glicemia alta:\n",
      "   paziente_id  etÃ  genere  glicemia diagnosi\n",
      "2            3   67      M       142  diabete\n",
      "4            5   71      M       170  diabete\n",
      "7            8   62      F       130  diabete\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Caricamento dati clinici di esempio\n",
    "df_clinici = pd.DataFrame({\n",
    "    'paziente_id': range(1, 11),\n",
    "    'etÃ ': [45, 52, 67, 38, 71, 49, 55, 62, 41, 58],\n",
    "    'genere': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F'],\n",
    "    'glicemia': [95, 105, 142, 88, 170, 98, 110, 130, 92, 115],\n",
    "    'pressione_sistolica': [120, 135, 155, 110, 165, 125, 140, 150, 118, 145],\n",
    "    'colesterolo': [180, 220, 250, 175, 270, 190, 230, 245, 185, 235],\n",
    "    'diagnosi': ['sano', 'pre-diabete', 'diabete', 'sano', 'diabete', \n",
    "                'sano', 'pre-diabete', 'diabete', 'sano', 'pre-diabete']\n",
    "})\n",
    "\n",
    "# Statistiche descrittive per gruppo diagnostico\n",
    "stats_per_diagnosi = df_clinici.groupby('diagnosi').agg({\n",
    "    'etÃ ': ['mean', 'std'],\n",
    "    'glicemia': ['mean', 'std'],\n",
    "    'pressione_sistolica': ['mean', 'std'],\n",
    "    'colesterolo': ['mean', 'std']\n",
    "})\n",
    "\n",
    "print(\"Statistiche per gruppo diagnostico:\")\n",
    "print(stats_per_diagnosi)\n",
    "\n",
    "# Correlazione tra parametri\n",
    "corr = df_clinici[['etÃ ', 'glicemia', 'pressione_sistolica', 'colesterolo']].corr()\n",
    "print(\"\\nMatrice di correlazione:\")\n",
    "print(corr)\n",
    "\n",
    "# Identifica pazienti con glicemia alta (>125)\n",
    "pazienti_glicemia_alta = df_clinici[df_clinici['glicemia'] > 125]\n",
    "print(\"\\nPazienti con glicemia alta:\")\n",
    "print(pazienti_glicemia_alta[['paziente_id', 'etÃ ', 'genere', 'glicemia', 'diagnosi']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ea6a70",
   "metadata": {},
   "source": [
    "UtilitÃ :\n",
    "- Pulizia nomenclatura diagnosi\n",
    "- Statistiche descrittive per reparto\n",
    "- Individuazione outlier (df[df[\"HbA1c\"]>12])\n",
    "- Esportazione rapida in Excel per il board clinico (stats.to_excel(\"summary.xlsx\"))\n",
    "\n",
    "{Tabella heat-map dei valori medi di HbA1c per diagnosi}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bba496",
   "metadata": {},
   "source": [
    "## Appunto sui file\n",
    "###Cosa sono\n",
    "\n",
    "Pandas eccelle nella gestione di file di dati, che Ã¨ uno dei suoi punti di forza principali. Questo codice include:\n",
    "- Lettura e scrittura di diversi formati file: CSV, Excel, TSV, JSON, SQL\n",
    "- Opzioni di lettura avanzate: gestione separatori, encoding, intestazioni personalizzate, - - gestione dei valori mancanti\n",
    "\n",
    "Un file Ã¨ un contenitore di dati salvati su disco (es. CSV, TXT, Excel, immagini, audio...)\n",
    "Python puÃ² aprire, leggere, modificare e salvare file, come se li â€œsfogliasse riga per rigaâ€ o li caricassi in memoria come tabelle.\n",
    ".\n",
    "\n",
    "### Modi principali per lavorare con file in Python\n",
    "1.File di testo (.txt, .csv, .tsv, .json, .log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb1c9870",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dati.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdati.txt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m      2\u001b[39m     contenuto = f.read()         \u001b[38;5;66;03m# Legge tutto il file come stringa\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_course2_materials/python_course_2/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:325\u001b[39m, in \u001b[36m_modified_open\u001b[39m\u001b[34m(file, *args, **kwargs)\u001b[39m\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m}:\n\u001b[32m    319\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    320\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIPython won\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m by default \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    321\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    322\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33myou can use builtins\u001b[39m\u001b[33m'\u001b[39m\u001b[33m open.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    323\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'dati.txt'"
     ]
    }
   ],
   "source": [
    "with open(\"dati.txt\", \"r\") as f:\n",
    "    contenuto = f.read()         # Legge tutto il file come stringa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f92e7",
   "metadata": {},
   "source": [
    "2. File strutturati come tabelle (CSV, Excel, TSV)\n",
    "Qui entra in gioco Pandas, che ti fa risparmiare ore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b248a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"pazienti.csv\")         # CSV = valori separati da virgole\n",
    "df = pd.read_excel(\"analisi.xlsx\")       # Legge un file Excel\n",
    "df = pd.read_json(\"dati.json\")           # Legge un file JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77a0b4e",
   "metadata": {},
   "source": [
    "Pandas riconosce automaticamente il formato dei dati, crea una tabella (DataFrame) e assegna intestazioni e indici.\n",
    "\n",
    "### Come si integrano i file con le librerie scientifiche\n",
    "| Obiettivo                | Funzione Python/Pandas            |\n",
    "| ------------------------ | --------------------------------- |\n",
    "| Leggere un file          | `open()`, `pd.read_csv()`, ecc.   |\n",
    "| Scrivere su file         | `write()`, `df.to_csv()`          |\n",
    "| Elaborare i dati         | Manipolazione tramite Pandas      |\n",
    "| Lavorare con file grandi | `chunksize`, `Parquet`, `feather` |\n",
    "\n",
    "Esempio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "818508a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"espressione_genica.tsv\", sep=\"\\t\")  # leggo da file\n",
    "df.to_csv(\"risultati_filtrati.csv\")                   # salvo su file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5967e6",
   "metadata": {},
   "source": [
    "### ðŸ§¬ 2 | Trascrittomica / RNA-seq\n",
    "Scenario:\n",
    "Analisi di dati di espressione genica per identificare geni differenzialmente espressi tra campioni di controllo e trattati."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06d1d11a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'counts_matrix.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# --- 1. Caricamento -----------------------------------------------------------\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m counts = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcounts_matrix.tsv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\t\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m   \u001b[38;5;66;03m# righe = gene_id\u001b[39;00m\n\u001b[32m      6\u001b[39m meta   = pd.read_csv(\u001b[33m\"\u001b[39m\u001b[33msamples.tsv\u001b[39m\u001b[33m\"\u001b[39m,       sep=\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[33m\"\u001b[39m)                \u001b[38;5;66;03m# sample_id, condition\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# --- 2. Selezione campioni ----------------------------------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_course2_materials/python_course_2/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_course2_materials/python_course_2/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_course2_materials/python_course_2/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_course2_materials/python_course_2/.venv/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/python_course2_materials/python_course_2/.venv/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'counts_matrix.tsv'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "# --- 1. Caricamento -----------------------------------------------------------\n",
    "counts = pd.read_csv(\"counts_matrix.tsv\", sep=\"\\t\", index_col=0)   # righe = gene_id\n",
    "meta   = pd.read_csv(\"samples.tsv\",       sep=\"\\t\")                # sample_id, condition\n",
    "\n",
    "# --- 2. Selezione campioni ----------------------------------------------------\n",
    "ctrl_cols  = meta.loc[meta[\"condition\"] == \"control\",  \"sample_id\"]\n",
    "treat_cols = meta.loc[meta[\"condition\"] == \"treated\",  \"sample_id\"]\n",
    "\n",
    "# --- 3. Normalizzazione CPM (counts-per-million) ------------------------------\n",
    "cpm = counts.div(counts.sum(axis=0), axis=1) * 1_000_000   # resta DataFrame\n",
    "\n",
    "# --- 4. Medie di espressione per condizione -----------------------------------\n",
    "mean_ctrl  = cpm[ctrl_cols].mean(axis=1)\n",
    "mean_treat = cpm[treat_cols].mean(axis=1)\n",
    "\n",
    "# --- 5. Log2 fold-change (solo Pandas + math) ---------------------------------\n",
    "log2fc = ((mean_treat + 1) / (mean_ctrl + 1)).apply(math.log2)\n",
    "\n",
    "# --- 6. DataFrame finale ------------------------------------------------------\n",
    "deg = pd.concat(\n",
    "    [mean_ctrl.round(2).rename(\"mean_ctrl\"),\n",
    "     mean_treat.round(2).rename(\"mean_treat\"),\n",
    "     log2fc.round(3).rename(\"log2FC\")],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# --- 7. Filtro preliminare sui candidati --------------------------------------\n",
    "candidati = deg[deg[\"log2FC\"].abs() > 1]\n",
    "\n",
    "# --- 8. Esportazione ----------------------------------------------------------\n",
    "candidati.to_csv(\"DEG_pre_screen.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c8270",
   "metadata": {},
   "source": [
    "UtilitÃ :\n",
    "- Lettura in Parquet â†’ RAM-efficient\n",
    "- Filtri basati su media/varianza\n",
    "- Calcolo di reti di co-espressione\n",
    "- Output diretto verso Seaborn/Scanpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7d5d7d",
   "metadata": {},
   "source": [
    "### ðŸ§  3 | Imaging e Quantificazione\n",
    "Scenario:\n",
    "Quantificazione di parametri morfometrici cellulari in risposta a diversi trattamenti farmacologici."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f485420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistiche morfometriche per trattamento:\n",
      "             area                                  perimetro            \\\n",
      "            count        mean       std  min  max       mean       std   \n",
      "trattamento                                                              \n",
      "controllo       7  100.000000  3.559026   95  105  34.428571  1.511858   \n",
      "farmaco_A       7   80.857143  2.794553   77   85  29.285714  1.380131   \n",
      "farmaco_B       6  128.666667  4.718757  122  135  41.500000  1.870829   \n",
      "\n",
      "            intensitÃ _nucleo            \n",
      "                        mean       std  \n",
      "trattamento                             \n",
      "controllo         120.000000  3.872983  \n",
      "farmaco_A         149.428571  4.755949  \n",
      "farmaco_B          99.666667  7.118052  \n",
      "\n",
      "Rapporto area/perimetro per trattamento:\n",
      "                 mean       std\n",
      "trattamento                    \n",
      "controllo    2.905942  0.059961\n",
      "farmaco_A    2.762589  0.047845\n",
      "farmaco_B    3.101391  0.027712\n",
      "\n",
      "Cellule con area superiore alla media + deviazione standard (121.98):\n",
      "    cell_id trattamento  area\n",
      "14       15   farmaco_B   125\n",
      "15       16   farmaco_B   130\n",
      "16       17   farmaco_B   128\n",
      "17       18   farmaco_B   135\n",
      "18       19   farmaco_B   122\n",
      "19       20   farmaco_B   132\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simuliamo dati di morfologia cellulare\n",
    "cell_data = pd.DataFrame({\n",
    "    'cell_id': range(1, 21),\n",
    "    'trattamento': ['controllo']*7 + ['farmaco_A']*7 + ['farmaco_B']*6,\n",
    "    'area': [100, 95, 105, 98, 102, 97, 103, \n",
    "             82, 78, 85, 80, 83, 77, 81, \n",
    "             125, 130, 128, 135, 122, 132],\n",
    "    'perimetro': [35, 33, 36, 34, 35, 32, 36,\n",
    "                  30, 28, 31, 29, 30, 27, 30,\n",
    "                  40, 42, 41, 44, 39, 43],\n",
    "    'intensitÃ _nucleo': [120, 115, 125, 118, 122, 116, 124,\n",
    "                        150, 145, 155, 148, 152, 142, 154,\n",
    "                        100, 95, 105, 90, 110, 98]\n",
    "})\n",
    "\n",
    "# Statistiche per gruppo di trattamento\n",
    "stats_morfologia = cell_data.groupby('trattamento').agg({\n",
    "    'area': ['count', 'mean', 'std', 'min', 'max'],\n",
    "    'perimetro': ['mean', 'std'],\n",
    "    'intensitÃ _nucleo': ['mean', 'std']\n",
    "})\n",
    "\n",
    "print(\"Statistiche morfometriche per trattamento:\")\n",
    "print(stats_morfologia)\n",
    "\n",
    "# Calcolo di feature derivate\n",
    "cell_data['ratio_area_perimetro'] = cell_data['area'] / cell_data['perimetro']\n",
    "\n",
    "# Statistiche della feature derivata\n",
    "ratio_stats = cell_data.groupby('trattamento')['ratio_area_perimetro'].agg(['mean', 'std'])\n",
    "print(\"\\nRapporto area/perimetro per trattamento:\")\n",
    "print(ratio_stats)\n",
    "\n",
    "# Identificazione delle cellule con area molto grande\n",
    "threshold = cell_data['area'].mean() + cell_data['area'].std()\n",
    "cellule_grandi = cell_data[cell_data['area'] > threshold]\n",
    "print(f\"\\nCellule con area superiore alla media + deviazione standard ({threshold:.2f}):\")\n",
    "print(cellule_grandi[['cell_id', 'trattamento', 'area']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1c6314",
   "metadata": {},
   "source": [
    "UtilitÃ :\n",
    "- Filtraggio rapido su soglia volumetrica\n",
    "- Calcolo delta-volume fra follow-up\n",
    "- Facilita il merge con outcome clinici"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b28af33",
   "metadata": {},
   "source": [
    "### ðŸ§« 4 | High-Content Screening (96-well, 384-well)\n",
    "Scenario:\n",
    "Screening di composti in piastra multi-well per identificare molecole attive attraverso saggi di viabilitÃ  cellulare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "072ed88e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dati screening:\n",
      "   well  compound  concentration_uM  viability_pct\n",
      "0    A1      DMSO                 0             98\n",
      "1    A2      DMSO                 0            102\n",
      "2    A3      DMSO                 0             97\n",
      "3    A4      DMSO                 0            103\n",
      "4    A5  Positivo                10             25\n",
      "5    A6  Positivo                10             22\n",
      "6    B1  Positivo                10             18\n",
      "7    B2  Positivo                10             20\n",
      "8    B3    Comp_1                 1             95\n",
      "9    B4    Comp_1                 3             85\n",
      "10   B5    Comp_1                10             70\n",
      "11   B6    Comp_1                30             30\n",
      "12   C1    Comp_2                 1            100\n",
      "13   C2    Comp_2                 3             98\n",
      "14   C3    Comp_2                10             95\n",
      "15   C4    Comp_2                30             92\n",
      "16   C5    Comp_3                 1             90\n",
      "17   C6    Comp_3                 3             60\n",
      "18   D1    Comp_3                10             40\n",
      "19   D2    Comp_3                30             20\n",
      "20   D3    Comp_4                 1             85\n",
      "21   D4    Comp_4                 3             80\n",
      "22   D5    Comp_4                10             50\n",
      "23   D6    Comp_4                30             35\n",
      "\n",
      "Media controllo negativo (DMSO): 100.00%\n",
      "Media controllo positivo: 21.25%\n",
      "\n",
      "Effetto dose-risposta (% normalizzata):\n",
      "concentration_uM          1          3          10         30\n",
      "compound                                                     \n",
      "Comp_1             93.650794  80.952381  61.904762  11.111111\n",
      "Comp_2            100.000000  97.460317  93.650794  89.841270\n",
      "Comp_3             87.301587  49.206349  23.809524  -1.587302\n",
      "Comp_4             80.952381  74.603175  36.507937  17.460317\n",
      "\n",
      "Composti attivi (hit) a 10 ÂµM (< 50% viabilitÃ  normalizzata):\n",
      "   compound  normalized_response\n",
      "18   Comp_3            23.809524\n",
      "22   Comp_4            36.507937\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Simulazione di dati di high-content screening (piastra 96-well)\n",
    "# Definizione della piastra (semplificata, solo 24 pozzetti)\n",
    "rows = list('ABCD')\n",
    "cols = list(range(1, 7))\n",
    "wells = [f\"{r}{c}\" for r in rows for c in cols]\n",
    "\n",
    "# Creazione dei dati\n",
    "hcs_data = pd.DataFrame({\n",
    "    'well': wells,\n",
    "    'compound': ['DMSO']*4 + ['Positivo']*4 + \n",
    "                ['Comp_1']*4 + ['Comp_2']*4 + ['Comp_3']*4 + ['Comp_4']*4,\n",
    "    'concentration_uM': [0]*4 + [10]*4 + [1, 3, 10, 30]*4,\n",
    "    'viability_pct': [\n",
    "        # DMSO (controllo negativo)\n",
    "        98, 102, 97, 103,\n",
    "        # Controllo positivo\n",
    "        25, 22, 18, 20,\n",
    "        # Comp_1\n",
    "        95, 85, 70, 30,\n",
    "        # Comp_2\n",
    "        100, 98, 95, 92,\n",
    "        # Comp_3\n",
    "        90, 60, 40, 20,\n",
    "        # Comp_4\n",
    "        85, 80, 50, 35\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"Dati screening:\")\n",
    "print(hcs_data)\n",
    "\n",
    "# Calcolo media dei controlli\n",
    "mean_neg_ctrl = hcs_data[hcs_data['compound'] == 'DMSO']['viability_pct'].mean()\n",
    "mean_pos_ctrl = hcs_data[hcs_data['compound'] == 'Positivo']['viability_pct'].mean()\n",
    "\n",
    "print(f\"\\nMedia controllo negativo (DMSO): {mean_neg_ctrl:.2f}%\")\n",
    "print(f\"Media controllo positivo: {mean_pos_ctrl:.2f}%\")\n",
    "\n",
    "# Normalizzazione rispetto ai controlli (0% = controllo positivo, 100% = DMSO)\n",
    "hcs_data['normalized_response'] = 100 * (hcs_data['viability_pct'] - mean_pos_ctrl) / (mean_neg_ctrl - mean_pos_ctrl)\n",
    "\n",
    "# Pivot table per visualizzare l'effetto dose-risposta\n",
    "dose_response = hcs_data[~hcs_data['compound'].isin(['DMSO', 'Positivo'])].pivot_table(\n",
    "    index='compound', \n",
    "    columns='concentration_uM', \n",
    "    values='normalized_response',\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "print(\"\\nEffetto dose-risposta (% normalizzata):\")\n",
    "print(dose_response)\n",
    "\n",
    "# Identificazione dei composti attivi (hit) a 10 ÂµM\n",
    "hits_10uM = hcs_data[(hcs_data['concentration_uM'] == 10) & \n",
    "                    (hcs_data['compound'] != 'DMSO') & \n",
    "                    (hcs_data['compound'] != 'Positivo') & \n",
    "                    (hcs_data['normalized_response'] < 50)]\n",
    "\n",
    "print(\"\\nComposti attivi (hit) a 10 ÂµM (< 50% viabilitÃ  normalizzata):\")\n",
    "print(hits_10uM[['compound', 'normalized_response']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fae3146",
   "metadata": {},
   "source": [
    "UtilitÃ \n",
    "- pivot_table rende il dataset â€œwideâ€ per calcolo Î”\n",
    "- Rank dei top-hits per successiva validazione\n",
    "- Integrare annotazioni chimiche con merge\n",
    "\n",
    "{Foto di piastra 384-well con pozzi colorimetrici + barplot crescita}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e59cdc2",
   "metadata": {},
   "source": [
    "### ðŸ’‰ 5. Studi di Coorte Longitudinali\n",
    "Scenario:\n",
    "Analisi longitudinale di pazienti in uno studio clinico per valutare l'efficacia di diversi trattamenti nel tempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4c9ec7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset longitudinale completo:\n",
      "   patient_id  visit_month  score  age gender treatment  baseline_score\n",
      "0           1            0     50   45      M         A              50\n",
      "1           2            0     55   52      F         A              55\n",
      "2           3            0     60   67      M         A              60\n",
      "3           4            0     45   38      F         A              45\n",
      "4           5            0     65   71      M         A              65\n",
      "5           6            0     52   49      F         B              52\n",
      "6           7            0     58   55      M         B              58\n",
      "7           8            0     63   62      F         B              63\n",
      "8           9            0     48   41      M         B              48\n",
      "9          10            0     56   58      F         B              56\n",
      "\n",
      "Andamento temporale per gruppo di trattamento:\n",
      "                        mean       std  count\n",
      "treatment visit_month                        \n",
      "A         0            55.00  7.905694      5\n",
      "          3            48.60  6.841053      5\n",
      "          6            41.25  2.986079      4\n",
      "          12           36.50  2.380476      4\n",
      "B         0            55.40  5.727128      5\n",
      "          3            53.00  5.291503      5\n",
      "          6            51.00  4.898979      5\n",
      "          12           49.20  4.086563      5\n",
      "\n",
      "Cambiamenti rispetto al baseline per paziente:\n",
      "   patient_id treatment  baseline  month12  change_12m\n",
      "0           1         A      50.0     35.0       -15.0\n",
      "1           2         A      55.0     36.0       -19.0\n",
      "2           3         A      60.0     40.0       -20.0\n",
      "3           4         A      45.0     35.0       -10.0\n",
      "4           5         A      65.0      NaN         NaN\n",
      "5           6         B      52.0     47.0        -5.0\n",
      "6           7         B      58.0     50.0        -8.0\n",
      "7           8         B      63.0     55.0        -8.0\n",
      "8           9         B      48.0     44.0        -4.0\n",
      "9          10         B      56.0     50.0        -6.0\n",
      "\n",
      "Cambiamento medio per gruppo di trattamento:\n",
      "           change_3m  change_6m  change_12m\n",
      "treatment                                  \n",
      "A               -6.4     -11.25       -16.0\n",
      "B               -2.4      -4.40        -6.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Creazione di un dataset longitudinale semplificato\n",
    "# Dati baseline (visita iniziale)\n",
    "baseline_data = pd.DataFrame({\n",
    "    'patient_id': range(1, 11),\n",
    "    'age': [45, 52, 67, 38, 71, 49, 55, 62, 41, 58],\n",
    "    'gender': ['M', 'F', 'M', 'F', 'M', 'F', 'M', 'F', 'M', 'F'],\n",
    "    'treatment': ['A', 'A', 'A', 'A', 'A', 'B', 'B', 'B', 'B', 'B'],\n",
    "    'baseline_score': [50, 55, 60, 45, 65, 52, 58, 63, 48, 56]\n",
    "})\n",
    "\n",
    "# Creazione dati di follow-up\n",
    "followup_data = []\n",
    "\n",
    "# Valori dei punteggi alle visite di follow-up\n",
    "# Paziente 1-5 (trattamento A)\n",
    "month3_scores_A = [45, 48, 52, 40, 58]\n",
    "month6_scores_A = [40, 42, 45, 38, 52]\n",
    "month12_scores_A = [35, 36, 40, 35, 48]\n",
    "\n",
    "# Paziente 6-10 (trattamento B)\n",
    "month3_scores_B = [50, 55, 60, 46, 54]\n",
    "month6_scores_B = [48, 52, 58, 45, 52]\n",
    "month12_scores_B = [47, 50, 55, 44, 50]\n",
    "\n",
    "# Costruzione dei dati di follow-up\n",
    "for visit in [3, 6, 12]:\n",
    "    for i in range(10):\n",
    "        patient_id = i + 1\n",
    "        \n",
    "        # Simula dropout (paziente 5 esce dallo studio dopo 3 mesi)\n",
    "        if patient_id == 5 and visit > 3:\n",
    "            continue\n",
    "            \n",
    "        # Assegna il punteggio in base al gruppo e alla visita\n",
    "        if patient_id <= 5:  # Gruppo A\n",
    "            if visit == 3:\n",
    "                score = month3_scores_A[patient_id-1]\n",
    "            elif visit == 6:\n",
    "                score = month6_scores_A[patient_id-1]\n",
    "            else:  # visit == 12\n",
    "                score = month12_scores_A[patient_id-1]\n",
    "        else:  # Gruppo B\n",
    "            if visit == 3:\n",
    "                score = month3_scores_B[patient_id-6]\n",
    "            elif visit == 6:\n",
    "                score = month6_scores_B[patient_id-6]\n",
    "            else:  # visit == 12\n",
    "                score = month12_scores_B[patient_id-6]\n",
    "                \n",
    "        followup_data.append({\n",
    "            'patient_id': patient_id,\n",
    "            'visit_month': visit,\n",
    "            'score': score\n",
    "        })\n",
    "\n",
    "# Creazione DataFrame follow-up\n",
    "followup_df = pd.DataFrame(followup_data)\n",
    "\n",
    "# Unione con i dati baseline\n",
    "long_data = pd.merge(\n",
    "    followup_df,\n",
    "    baseline_data[['patient_id', 'age', 'gender', 'treatment', 'baseline_score']],\n",
    "    on='patient_id'\n",
    ")\n",
    "\n",
    "# Aggiunta del timepoint baseline (mese 0)\n",
    "baseline_for_merge = baseline_data.copy()\n",
    "baseline_for_merge['visit_month'] = 0\n",
    "baseline_for_merge['score'] = baseline_for_merge['baseline_score']\n",
    "baseline_for_merge = baseline_for_merge[['patient_id', 'visit_month', 'score', \n",
    "                                        'age', 'gender', 'treatment', 'baseline_score']]\n",
    "\n",
    "# Dataset completo con tutti i timepoint\n",
    "full_data = pd.concat([baseline_for_merge, long_data], ignore_index=True)\n",
    "\n",
    "print(\"Dataset longitudinale completo:\")\n",
    "print(full_data.head(10))\n",
    "\n",
    "# Analisi dell'andamento temporale per gruppo di trattamento\n",
    "temporal_means = full_data.groupby(['treatment', 'visit_month'])['score'].agg(['mean', 'std', 'count'])\n",
    "print(\"\\nAndamento temporale per gruppo di trattamento:\")\n",
    "print(temporal_means)\n",
    "\n",
    "# Calcolo del cambiamento rispetto al baseline\n",
    "# Pivot per avere i punteggi di ogni visita come colonne\n",
    "patient_changes = full_data.pivot_table(\n",
    "    index='patient_id', \n",
    "    columns='visit_month', \n",
    "    values='score'\n",
    ")\n",
    "\n",
    "# Rinomina le colonne per chiarezza\n",
    "patient_changes = patient_changes.rename(columns={\n",
    "    0: 'baseline', \n",
    "    3: 'month3', \n",
    "    6: 'month6', \n",
    "    12: 'month12'\n",
    "})\n",
    "\n",
    "# Calcola i cambiamenti\n",
    "patient_changes['change_3m'] = patient_changes['month3'] - patient_changes['baseline']\n",
    "patient_changes['change_6m'] = patient_changes['month6'] - patient_changes['baseline']\n",
    "patient_changes['change_12m'] = patient_changes['month12'] - patient_changes['baseline']\n",
    "\n",
    "# Reset dell'indice per unione\n",
    "patient_changes = patient_changes.reset_index()\n",
    "\n",
    "# Unione con caratteristiche baseline\n",
    "patient_summary = pd.merge(\n",
    "    patient_changes,\n",
    "    baseline_data[['patient_id', 'treatment', 'age', 'gender']],\n",
    "    on='patient_id'\n",
    ")\n",
    "\n",
    "print(\"\\nCambiamenti rispetto al baseline per paziente:\")\n",
    "print(patient_summary[['patient_id', 'treatment', 'baseline', 'month12', 'change_12m']])\n",
    "\n",
    "# Analisi per gruppo di trattamento\n",
    "treatment_changes = patient_summary.groupby('treatment')[['change_3m', 'change_6m', 'change_12m']].mean()\n",
    "print(\"\\nCambiamento medio per gruppo di trattamento:\")\n",
    "print(treatment_changes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7673236",
   "metadata": {},
   "source": [
    "UtilitÃ :\n",
    "- Ordinamento e calcolo di intervalli temporali\n",
    "- Analisi per paziente o sottogruppo\n",
    "- Calcolo della progressione (delta valori nel tempo)\n",
    "\n",
    "{Grafico temporale con andamento di biomarcatori in un paziente nel tempo}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38490599",
   "metadata": {},
   "source": [
    "# Numpy\n",
    "\n",
    "NumPy Ã¨ una libreria Python che fornisce una struttura dati semplice ma potente: l'array n-dimensionale.\n",
    "\n",
    "**PerchÃ© scegliere NumPy**\n",
    "Pur conoscendo giÃ  Python â€œpuroâ€ (con i suoi cicli `for`, la lettura/scrittura di CSV, ecc.), NumPy introduce un paradigma che offre vantaggi concreti:\n",
    "\n",
    "1. **Maggiore velocitÃ **\n",
    "\n",
    "   * NumPy utilizza algoritmi implementati in C, che eseguono operazioni in nanosecondi anzichÃ© in secondi.\n",
    "2. **Riduzione dei cicli**\n",
    "\n",
    "   * Grazie alle strutture array, Ã¨ possibile comporre operazioni vettoriali che eliminano gran parte dei loop manuali e lâ€™indice di iterazione.\n",
    "3. **Codice piÃ¹ leggibile**\n",
    "\n",
    "   * Senza cicli annidati, le espressioni nel codice assomigliano molto di piÃ¹ alle equazioni matematiche che si vogliono calcolare.\n",
    "4. **Alta qualitÃ  e affidabilitÃ **\n",
    "\n",
    "   * Unâ€™ampia comunitÃ  di sviluppatori mantiene NumPy veloce, di facile utilizzo e privo di bug.\n",
    "\n",
    "**Conclusione**\n",
    "Questi fattori hanno reso NumPy lo standard â€œde factoâ€ per gli array multidimensionali in Python applicato alla data science. Molte librerie popolari si basano su NumPy: impararlo fornisce una solida base su cui poi sviluppare competenze piÃ¹ avanzate in ambiti specifici.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b1f7e0",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.1 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.2.5\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# attiviamo l'ambiente virutale\n",
    ". .venv/bin/activate\n",
    "# Installiamo numpy\n",
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "caa2cd4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[72 35 64 88 51 90 74 12]\n",
      "[72, 35, 64, 88, 51, 90, 74, 12]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "CURVE_CENTER = 80 # Sintassi standard per le costanti\n",
    "grades = np.array([72, 35, 64, 88, 51, 90, 74, 12])\n",
    "grades_list = [72, 35, 64, 88, 51, 90, 74, 12]\n",
    "\n",
    "print(grades)\n",
    "print(grades_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8333435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(grades))\n",
    "print(type(grades_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c039cf47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 91.25,  54.25,  83.25, 100.  ,  70.25, 100.  ,  93.25,  31.25])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def curve(grades):\n",
    "     average = grades.mean()\n",
    "     change = CURVE_CENTER - average\n",
    "     new_grades = grades + change\n",
    "     return np.clip(new_grades, grades, 100)\n",
    "\n",
    "curve(grades)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cee3527",
   "metadata": {},
   "source": [
    "1. **Importazione di NumPy**\n",
    "   Alla riga 1 si importa la libreria NumPy con lâ€™alias `np`, una convenzione che abbrevia i comandi successivi.\n",
    "\n",
    "2. **Creazione dellâ€™array**\n",
    "   Alla riga 3 si definisce un array monodimensionale chiamato `grades` di lunghezza 8 e tipo `int64`. In seguito esplorerai forma e tipo dei dati piÃ¹ a fondo, ma per ora basti sapere che hai un vettore di 8 valori interi.\n",
    "\n",
    "3. **Calcolo della media**\n",
    "   Alla riga 5 si richiama il metodo `.mean()` sullâ€™array: in un solo passaggio NumPy somma tutti gli elementi e ne restituisce la media. Gli array di NumPy dispongono di numerosi metodi analoghi per operazioni statistiche o matematiche.\n",
    "\n",
    "4. **Vectorization e Broadcasting**\n",
    "   Alla riga 7 si sfruttano due concetti chiave:\n",
    "\n",
    "   * **Vectorization**: lâ€™operazione (es. somma di uno stesso valore) viene applicata simultaneamente a tutti gli elementi dellâ€™array, eliminando la necessitÃ  di cicli espliciti.\n",
    "   * **Broadcasting**: NumPy â€œallineaâ€ array di forme diverse per permettere calcoli vettoriali fra loro. Qui `grades` Ã¨ un array shape `(8,)` e `change` Ã¨ uno scalare shape `(1,)`; NumPy aggiunge automaticamente `change` a ciascun elemento di `grades`. Non avrebbe funzionato se grades fosse stata una lista!\n",
    "\n",
    "5. **Clipping dei valori**\n",
    "   Alla riga 8 si utilizza la funzione `np.clip()`, che garantisce che i voti â€œcurveâ€‘datiâ€ non scendano sotto un minimo nÃ© superino un massimo.\n",
    "\n",
    "   * Il secondo argomento di `clip()` Ã¨ proprio lâ€™array originale `grades`: cosÃ¬ ogni voto corretto non scende mai al di sotto del suo valore iniziale.\n",
    "   * Il terzo argomento Ã¨ lo scalare `100`, che tramite broadcasting assicura che nessun voto ecceda il 100%.\n",
    "\n",
    "> **Consiglio**: NumPy mette a disposizione decine di funzioni e metodi specializzati. Se ti sembra di ripetere operazioni comuni o di dover scrivere cicli, consulta la documentazione: molto probabilmente esiste giÃ  una routine adatta.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b845b4b9",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "# installiamo matplotlib\n",
    "pip install numpy matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79838613",
   "metadata": {},
   "source": [
    "## Forme degli array (Shape)\n",
    "\n",
    "* **Shape** Ã¨ la tupla che descrive la dimensione di ciascun asse di un array.\n",
    "* Ogni array NumPy espone la proprietÃ  `.shape`, che restituisce tale tupla.\n",
    "* Ãˆ fondamentale che gli array abbiano la forma che le funzioni si aspettano: un controllo rapido Ã¨ stampare lâ€™array insieme a `array.shape`.\n",
    "\n",
    "**Esempio: creazione e verifica di un array 3D 2Ã—2Ã—3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee7c8d29",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[[29.3 42.1 18.8]\n",
      "  [16.1 38.  12.5]]\n",
      "\n",
      " [[12.6 49.9 38.6]\n",
      "  [31.3  9.2 22.2]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Creo un vettore di 12 temperature e lo rimodello in un blocco 2Ã—2Ã—3\n",
    "temperatures = np.array([\n",
    "    29.3, 42.1, 18.8, 16.1, 38.0, 12.5,\n",
    "    12.6, 49.9, 38.6, 31.3,  9.2, 22.2\n",
    "]).reshape(2, 2, 3)\n",
    "# Verifico la shape\n",
    "print(temperatures.shape)\n",
    "\n",
    "# Visualizzo lâ€™array\n",
    "print(temperatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7be6ac2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(22.2)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Supponiamo di voler selezionare la temperatura 22.2\n",
    "temperature = temperatures[1, 1, 2]\n",
    "# [\"tabella\", \"riga\", \"colonn\"]\n",
    "temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cabb2cc",
   "metadata": {},
   "source": [
    "* Con tre o piÃ¹ dimensioni diventa difficile â€œvedereâ€ i dati; per questo Ã¨ utile\n",
    "\n",
    "  * ribaltare (â€œswapâ€) assi con `.swapaxes()`\n",
    "  * stampare shape e contenuto finchÃ© non si Ã¨ certi della disposizione.\n",
    "\n",
    "**Esempio: scambiare lâ€™asse 1 con lâ€™asse 2**\n",
    "\n",
    "```python\n",
    "reordered = np.swapaxes(temperatures, 1, 2)\n",
    "print(reordered)\n",
    "# Output:\n",
    "# array([[[29.3, 16.1],\n",
    "#         [42.1, 38. ],\n",
    "#         [18.8, 12.5]],\n",
    "#\n",
    "#        [[12.6, 31.3],\n",
    "#         [49.9,  9.2],\n",
    "#         [38.6, 22.2]]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Assi (Axes)\n",
    "\n",
    "* Gli **assi** indicano le dimensioni ed sono indicizzati da 0 in su.\n",
    "\n",
    "  * In un array 2D, `axis=0` Ã¨ lâ€™asse verticale (righe), `axis=1` quello orizzontale (colonne).\n",
    "* Molte funzioni NumPy cambiano comportamento a seconda dellâ€™argomento `axis`.\n",
    "\n",
    "**Esempio con `.max()`**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "table = np.array([\n",
    "    [5, 3, 7, 1],\n",
    "    [2, 6, 7, 9],\n",
    "    [1, 1, 1, 1],\n",
    "    [4, 3, 2, 0],\n",
    "])\n",
    "\n",
    "# Massimo sullâ€™intero array\n",
    "print(table.max())        # 9\n",
    "\n",
    "# Massimo per ciascuna colonna (asse 0)\n",
    "print(table.max(axis=0))  # array([5, 6, 7, 9])\n",
    "\n",
    "# Massimo per ciascuna riga (asse 1)\n",
    "print(table.max(axis=1))  # array([7, 9, 1, 4])\n",
    "```\n",
    "\n",
    "* **Senza `axis`** â†’ funzione su tutti i valori.\n",
    "* **Con `axis=k`** â†’ operazione lungo lâ€™asse k, restituisce un array con dimensione ridotta di 1.\n",
    "\n",
    "---\n",
    "\n",
    "## Broadcasting\n",
    "\n",
    "La regola fondamentale:\n",
    "\n",
    "> Due array possono â€œbroadcastarsiâ€ se, per ogni asse, o hanno la stessa lunghezza, oppure almeno uno dei due Ã¨ pari a 1.\n",
    "\n",
    "* Se in un asse una dimensione vale 1, NumPy **duplica** quel dato lungo quellâ€™asse.\n",
    "* Se le dimensioni coincidono, operazione elemento-per-elemento.\n",
    "\n",
    "**Esempio formale:**\n",
    "\n",
    "* `A.shape = (4, 1, 8)`\n",
    "* `B.shape = (1, 6, 8)`\n",
    "\n",
    "| Asse   | A | B | Azione                 |\n",
    "| ------ | - | - | ---------------------- |\n",
    "| asse 0 | 4 | 1 | B duplicato 4 volte    |\n",
    "| asse 1 | 1 | 6 | A duplicato 6 volte    |\n",
    "| asse 2 | 8 | 8 | dimensioni uguali â†’ OK |\n",
    "\n",
    "**Creazione degli array**\n",
    "\n",
    "```python\n",
    "A = np.arange(32).reshape(4, 1, 8)\n",
    "B = np.arange(48).reshape(1, 6, 8)\n",
    "```\n",
    "\n",
    "**Somma con broadcasting**\n",
    "\n",
    "```python\n",
    "C = A + B\n",
    "print(C)\n",
    "# Output:\n",
    "# array([[[ 0,  2,  4,  6,  8, 10, 12, 14],\n",
    "#         [ 8, 10, 12, 14, 16, 18, 20, 22],\n",
    "#         [16, 18, 20, 22, 24, 26, 28, 30],\n",
    "#         [24, 26, 28, 30, 32, 34, 36, 38],\n",
    "#         [32, 34, 36, 38, 40, 42, 44, 46],\n",
    "#         [40, 42, 44, 46, 48, 50, 52, 54]],\n",
    "#\n",
    "#        [[ 8, 10, 12, 14, 16, 18, 20, 22],\n",
    "#         ...\n",
    "#        ],\n",
    "#        ...\n",
    "# ])\n",
    "```\n",
    "\n",
    "* NumPy estende internamente A e B alle stesse dimensioni 4Ã—6Ã—8, poi somma elemento per elemento.\n",
    "\n",
    "---\n",
    "\n",
    "### Conclusione\n",
    "\n",
    "* **Shape** e **axes** sono la base per navigare array multidimensionali.\n",
    "* Controlla sempre `.shape` e, quando serve, usa `.swapaxes()` o funzioni con `axis=`.\n",
    "* Comprendere il **broadcasting** permette di scrivere calcoli vettoriali puliti e veloci, evitando loop espliciti.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
